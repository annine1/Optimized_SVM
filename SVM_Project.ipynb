{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0i9AaWwD040"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split \n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/mohammedelabbas/SVM/main/Preprocessing.py\n",
        "!wget https://raw.githubusercontent.com/mohammedelabbas/SVM/main/svm.py\n",
        "!wget https://raw.githubusercontent.com/mohammedelabbas/SVM/main/word_representation.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2AOlTIL5-u-",
        "outputId": "2e967d5f-c9b8-43fd-9430-8547e3a27123"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-22 08:14:07--  https://raw.githubusercontent.com/mohammedelabbas/SVM/main/Preprocessing.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1270 (1.2K) [text/plain]\n",
            "Saving to: ‘Preprocessing.py’\n",
            "\n",
            "\rPreprocessing.py      0%[                    ]       0  --.-KB/s               \rPreprocessing.py    100%[===================>]   1.24K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-04-22 08:14:07 (91.9 MB/s) - ‘Preprocessing.py’ saved [1270/1270]\n",
            "\n",
            "--2022-04-22 08:14:08--  https://raw.githubusercontent.com/mohammedelabbas/SVM/main/svm.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1239 (1.2K) [text/plain]\n",
            "Saving to: ‘svm.py’\n",
            "\n",
            "svm.py              100%[===================>]   1.21K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-04-22 08:14:08 (62.4 MB/s) - ‘svm.py’ saved [1239/1239]\n",
            "\n",
            "--2022-04-22 08:14:08--  https://raw.githubusercontent.com/mohammedelabbas/SVM/main/word_representation.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3255 (3.2K) [text/plain]\n",
            "Saving to: ‘word_representation.py’\n",
            "\n",
            "word_representation 100%[===================>]   3.18K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-04-22 08:14:08 (41.1 MB/s) - ‘word_representation.py’ saved [3255/3255]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epxTwNUVyf_e"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "hJMtlJdkFEJM",
        "outputId": "69144e9d-63b5-4f06-dbb8-59fd94373da1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Phil the Alien is one of those quirky films where the humour is based around the oddness of everything rather than actual punchlines.<br /><br />At first it was very odd and pretty funny but as the movie progressed I didn\\'t find the jokes or oddness funny anymore.<br /><br />Its a low budget film (thats never a problem in itself), there were some pretty interesting characters, but eventually I just lost interest.<br /><br />I imagine this film would appeal to a stoner who is currently partaking.<br /><br />For something similar but better try \"Brother from another planet\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
        "df[\"review\"][10]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"sentiment\"] [df[\"sentiment\"] == \"positive\"] = 1\n",
        "df[\"sentiment\"] [df[\"sentiment\"] == \"negative\"] = -1\n",
        "labels  = df[\"sentiment\"]\n",
        "labels = np.array(labels)"
      ],
      "metadata": {
        "id": "u4G4rd3Ge3gr"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "class DataPreprocessing:\n",
        "  \n",
        "  def _remove_html(self, data):\n",
        "    for i in range(len(data)) :\n",
        "      p = re.compile(r'<.*?>')\n",
        "      data[i] = p.sub('', data[i])\n",
        "      \n",
        "    return data\n",
        "\n",
        "  #lowercase\n",
        "  def _convert_to_lower_case(self,data):\n",
        "    for i in range(len(data)):\n",
        "      data[i] = data[i].lower()\n",
        "    return data  \n",
        "\n",
        "  #single characters\n",
        "  def _remove_single_characters(self, data):\n",
        "    for i in range(len(data)) :  \n",
        "      data[i] = \" \".join([w for w in data[i].split() if len(w)>1]) \n",
        "    return data \n",
        "\n",
        "   #punctuation  \n",
        "  def _remove_punctuation(self,data):\n",
        "    symbols = \"!,\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~'\\n\"\n",
        "    for symbol in symbols:\n",
        "      for i in range(len(data)):\n",
        "        data[i] = data[i].replace(symbol,\"\")\n",
        "    return data  \n",
        "\n",
        "  #stemming  \n",
        "  def _stem(self,data):\n",
        "    ps = PorterStemmer ()\n",
        "    \n",
        "    for i in range(len(data)) :\n",
        "      new_doc = \"\"\n",
        "      for word in data[i].split():\n",
        "        new_doc+= ps.stem(word)+\" \"\n",
        "      data[i] = new_doc  \n",
        "    return data   \n",
        "  \n",
        "  def preprocess(self,data):\n",
        "     data = self._remove_html(data)\n",
        "     data = self._remove_single_characters(data)\n",
        "     data = self._convert_to_lower_case(data)\n",
        "     data = self._remove_punctuation(data)\n",
        "     data = self._stem(data)\n",
        "\n",
        "     return data"
      ],
      "metadata": {
        "id": "PiwHQ6NrmULE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yet616heEK3p"
      },
      "source": [
        "# Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "#from Preprocessing import DataPreprocessing\n",
        "preprocessing = DataPreprocessing()\n",
        "data = preprocessing.preprocess(list(df[\"review\"]))"
      ],
      "metadata": {
        "id": "kq7FX_wOb7uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NI157UAbl7ce"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O3QYEikEXXs"
      },
      "source": [
        "# Word Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Count Frequency Encoding**"
      ],
      "metadata": {
        "id": "KPvU33Z3nkA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class WordFrequency:\n",
        "  def _get_word_set(self,text): \n",
        "    sentences = []\n",
        "    word_set = []\n",
        "    for sent in text:\n",
        "        x = [i.lower() for  i in sent.split() if i.isalpha()]\n",
        "        sentences.append(x)\n",
        "        for word in x:\n",
        "            if word not in word_set:\n",
        "                word_set.append(word)\n",
        "    return set(word_set),sentences\n",
        "  def _count_word(self,word,sentence):\n",
        "    count = 0\n",
        "    for w in sentence:\n",
        "      if w==word:\n",
        "        count +=1\n",
        "    return count    \n",
        "\n",
        "  def word_frequency(self,text):\n",
        "    word_set , sentences = self._get_word_set(text) \n",
        "    print (sorted(word_set))\n",
        "    word_count = []\n",
        "    for sentence in sentences:\n",
        "      row =[]\n",
        "      for word in sorted(word_set):\n",
        "        row.append(self._count_word(word,sentence))\n",
        "      word_count.append(row)\n",
        "    return np.array(word_count)\n",
        "\n",
        "  \n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "class WordRepresentation:\n",
        "  def __init__(self, method = 'tf-idf'):\n",
        "    self.method = method\n",
        "  def count_frequency(self,text):\n",
        "      wf = WordFrequency()\n",
        "      return wf.word_frequency(text)\n",
        "\n",
        "  def tf_idf(self,text):\n",
        "    return TFIDF().tf_idf(text)     \n",
        "  def get_representation(self,text):\n",
        "    if self.method == \"tf-idf\":\n",
        "      return self.tf_idf(text)\n",
        "      \n",
        "    elif self.method==\"frequency-count\":\n",
        "      return self.count_frequency(text)\n",
        "    else:\n",
        "      raise Exception(\"Unknown method\")\n",
        "    pass\n",
        "\n"
      ],
      "metadata": {
        "id": "CMo11gEbmpKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF**"
      ],
      "metadata": {
        "id": "6ZCJU1fYnwNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TFIDF:\n",
        "\n",
        "  def _get_word_set(self,text): \n",
        "    sentences = []\n",
        "    word_set = []\n",
        "    for sent in text:\n",
        "        x = [i.lower() for  i in sent.split() if i.isalpha()]\n",
        "        sentences.append(x)\n",
        "        for word in x:\n",
        "            if word not in word_set:\n",
        "                word_set.append(word)\n",
        "    return set(word_set),sentences          \n",
        "\n",
        "  \n",
        "\n",
        "  def _count_dict(self,sentences,word_set):\n",
        "      word_count = {}\n",
        "      for word in word_set:\n",
        "          word_count[word] = 0\n",
        "          for sent in sentences:\n",
        "              if word in sent:\n",
        "                  word_count[word] += 1\n",
        "      return word_count\n",
        "  \n",
        "  #word_count = count_dict(sentences)    \n",
        "  #Term Frequency\n",
        "  def _termfreq(self,document, word):\n",
        "      N = len(document)\n",
        "      occurance = len([token for token in document if token == word])\n",
        "      return occurance/N\n",
        "\n",
        "\n",
        "  def _inverse_doc_freq(self,word,word_count,total_documents):\n",
        "      try:\n",
        "          word_occurance = word_count[word] + 1\n",
        "      except:\n",
        "          word_occurance = 1\n",
        "      return np.log(total_documents/word_occurance)\n",
        "\n",
        "  def tf_idf(self,text):\n",
        "    word_set,sentences = self._get_word_set(text)\n",
        "    #print (word_set)\n",
        "    total_documents = len(sentences)\n",
        "    #Creating an index for each word in our vocab.\n",
        "    index_dict = {} #Dictionary to store index for each word\n",
        "    i = 0\n",
        "    for word in word_set:\n",
        "      index_dict[word] = i\n",
        "      i += 1\n",
        "    #print (index_dict)  \n",
        "    \n",
        "    word_count = self._count_dict(sentences,word_set)\n",
        "    print (len(word_count))\n",
        "    vectors = []\n",
        "    print (len(sentences))\n",
        "    for sentence in sentences:\n",
        "      tf_idf_vec = np.zeros((len(word_set),))\n",
        "      for word in sentence:\n",
        "          #print (word)\n",
        "          tf = self._termfreq(sentence,word)\n",
        "          idf = self._inverse_doc_freq(word,word_count,total_documents)\n",
        "          value = tf*idf\n",
        "          tf_idf_vec[index_dict[word]] = value \n",
        "      vectors.append(tf_idf_vec)    \n",
        "    return np.array(vectors)   \n",
        "\n"
      ],
      "metadata": {
        "id": "y5rgiltxnwsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from word_representation import WordRepresentation\n",
        "repr = WordRepresentation() #method = \"frequency-count\"\n",
        "x= repr.tf_idf(data)\n",
        "x.shape"
      ],
      "metadata": {
        "id": "4CWI0zECd4qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x,labels[:100], test_size = 0.1, random_state = 10)\n",
        "x_train = x_train.astype(np.double)\n",
        "y_train = y_train.astype(np.double)\n",
        "x_test = x_test.astype(np.double)\n",
        "y_test = y_test.astype(np.double)\n",
        "print (x_train.shape)"
      ],
      "metadata": {
        "id": "7TZh3hkhfB2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkMs8Ku073QY"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xygnM-QMEbp3"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA3ukeoBWl4G",
        "outputId": "9f6513ee-02ac-479a-9b1b-0cdba9e5dad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cvxopt in /usr/local/lib/python3.7/dist-packages (1.2.7)\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pip install cvxopt\n",
        "from cvxopt import matrix as cvxopt_matrix\n",
        "from cvxopt import solvers\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install cvxopt\n",
        "from cvxopt import matrix as cvxopt_matrix\n",
        "from cvxopt import solvers\n",
        "import numpy as np\n",
        "\n",
        "class SVM:\n",
        "  def __init__(self):\n",
        "    self.w = None\n",
        "    self.b = None\n",
        "  def train(self,X,y):\n",
        "    m,n = X.shape\n",
        "    y = y.reshape(-1,1) * 1.\n",
        "    X_dash = y * X\n",
        "    H = np.dot(X_dash , X_dash.T) * 1\n",
        "\n",
        "    #Converting into cvxopt format\n",
        "    P = cvxopt_matrix(H)\n",
        "    q = cvxopt_matrix(-np.ones((m, 1)))\n",
        "    G = cvxopt_matrix(-np.eye(m))\n",
        "    h = cvxopt_matrix(np.zeros(m))\n",
        "    A = cvxopt_matrix(y.reshape(1, -1))\n",
        "    b = cvxopt_matrix(np.zeros(1))\n",
        "    solution = solvers.qp(P, q, G, h, A, b)\n",
        "    alpha = np.array(solution['x'])\n",
        "    w = ((alpha * y).T @X).reshape(-1, 1)\n",
        "    seuil = 0.0\n",
        "    s = (alpha > seuil).flatten()\n",
        "    b = y[s] - np.dot(X[s], w)\n",
        "    self.w = w\n",
        "    self.b =b\n",
        "    return w, b \n",
        "  def test(self,X):\n",
        "    print (X.shape)\n",
        "    n = X.shape[0]\n",
        "    y_pred = np.zeros((n,1))\n",
        "\n",
        "    for i in range(n):\n",
        "      distance=np.dot(np.array(X[i]), self.w) + self.b\n",
        "    #print (distance.shape)\n",
        "      y_pred[i] = np.sign(distance)[0]\n",
        "    return y_pred\n",
        "\n",
        "  def accuracy(self,y_pred,y):\n",
        "      n= y_pred.shape[0]\n",
        "      good_pred = 0.0\n",
        "      for i in range(n):\n",
        "        if y_pred[i] == y[i]:\n",
        "          good_pred +=1\n",
        "      return good_pred/n    "
      ],
      "metadata": {
        "id": "iqa5y4Z0mYx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from svm import SVM\n",
        "model = SVM()\n",
        "y = model.train(x_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "uRnuJiMDoiLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "UjR8-pKLgo9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.test(x_test)\n",
        "model.accuracy(y_pred,y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUKMQGNUiESR",
        "outputId": "f92a2401-b760-4746-fae7-03d922286aad"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 3980)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualization"
      ],
      "metadata": {
        "id": "Buh5XEK7cpiw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "SfeXE_8sWyvJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "76261e95-ec90-48b5-e599-3e461d0f72eb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-13671e72518e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Plotting our two-features-space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m sns.scatterplot(x=X_train[:, 0], \n\u001b[0m\u001b[1;32m      4\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "# Plotting our two-features-space\n",
        "sns.scatterplot(x=X_train[:, 0], \n",
        "                y=X_train[:, 1], \n",
        "                hue=y_train, \n",
        "                s=8);\n",
        "# Constructing a hyperplane using a formula.\n",
        "w = svc_model.coe          # w consists of 2 elements\n",
        "b = svc_model.intercept_[0]      # b consists of 1 element\n",
        "x_points = np.linspace(-1, 1)    # generating x-points from -1 to 1\n",
        "y_points = -(w[0] / w[1]) * x_points - b / w[1]  # getting corresponding y-points\n",
        "# Plotting a red hyperplane\n",
        "plt.plot(x_points, y_points, c='r');\n",
        "\n",
        "# Encircle support vectors\n",
        "# plt.scatter(svc_model.support_vectors_[:, 0],\n",
        "#             svc_model.support_vectors_[:, 1], \n",
        "#             s=50, \n",
        "#             facecolors='none', \n",
        "#             edgecolors='k', \n",
        "#             alpha=.5);\n",
        "\n",
        "# Step 2 (unit-vector):\n",
        "w_hat = Svmoptim_[0] / (np.sqrt(np.sum(SvmOptim_[0] ** 2)))\n",
        "# Step 3 (margin):\n",
        "margin = 1 / np.sqrt(np.sum(svc_model.coef_[0] ** 2))\n",
        "# Step 4 (calculate points of the margin lines):\n",
        "decision_boundary_points = np.array(list(zip(x_points, y_points)))\n",
        "points_of_line_above = decision_boundary_points + w_hat * margin\n",
        "points_of_line_below = decision_boundary_points - w_hat * margin\n",
        "# Plot margin lines\n",
        "# Blue margin line above\n",
        "plt.plot(points_of_line_above[:, 0], \n",
        "         points_of_line_above[:, 1], \n",
        "         'b--', \n",
        "         linewidth=2)\n",
        "# Green margin line below\n",
        "plt.plot(points_of_line_below[:, 0], \n",
        "         points_of_line_below[:, 1], \n",
        "         'g--',\n",
        "         linewidth=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Q8M1SABJoHNh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SVM Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}